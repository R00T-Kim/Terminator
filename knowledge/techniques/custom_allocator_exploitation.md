# Custom Allocator Exploitation Techniques

## Overview
Custom heap allocators bypass standard glibc protections (tcache, fastbin checks, safe-unlinking) but introduce their own attack surfaces. This document covers patterns observed in CTF challenges with non-standard allocators (hunter, Sand_Message) and general exploitation strategies.

## 1. Reconnaissance Phase (CRITICAL — Do This First)

### Identify the Allocator Type
```
| Indicator | Allocator Type | Example |
|-----------|---------------|---------|
| mmap + custom metadata | Fully custom | Sand_Message |
| Wrapper around malloc with randomization | Randomized wrapper | hunter |
| slab/pool with fixed-size slots | Slab allocator | kernel challenges |
| bump/arena allocator | Arena | WASM/embedded |
```

### Key Questions to Answer
1. **Chunk metadata layout**: Where is size/flags stored? Header before data or separate table?
2. **Free list structure**: Linked list? Bitmap? RB-tree? Bins by size class?
3. **Coalescing behavior**: Does it merge adjacent free chunks? How?
4. **Alignment requirements**: Forced alignment? Random offset?
5. **Validation checks**: What integrity checks exist on free/alloc?

### Reverse Engineering Checklist
```bash
# r2: Find allocator functions
r2 -A ./binary
afl~alloc
afl~free
afl~malloc
afl~heap

# GDB: Trace allocations
break *custom_malloc
commands
  silent
  printf "malloc(%d) = %p\n", $rdi, $rax
  continue
end
```

## 2. Randomized Wrapper Allocators (hunter pattern)

### Structure
```c
void* custom_malloc(size_t size) {
    malloc(random_padding);       // noise allocation (discarded)
    void* p = malloc(size + N);   // actual allocation
    return p + random_offset;     // shifted pointer
}
void custom_free(void* ptr) {
    free(ptr & ALIGN_MASK);       // mask off random offset
}
```

### Attack Strategies

#### A. Size Class Matching
The wrapper adds padding (`size + N`), so the real malloc size differs from the request.
- **Calculate real chunk sizes**: `request + N + malloc_overhead → round to bin size`
- **Match freed chunk to new allocation**: Find operations that allocate the SAME real chunk size
- hunter example: player=`malloc(0x18)`→0x20 chunk, cmd=`malloc(0x19)`→0x28 chunk → **mismatch!**

#### B. Probabilistic Brute Force (rand_offset=0)
When `random_offset ∈ {0,1,2,3}`:
- `rand_offset == 0` probability = **25%** per allocation
- Exploitation requiring aligned pointer: brute force ~4 attempts average
- **Strategy**: Wrap exploit in retry loop with timeout
```python
from pwn import *
for attempt in range(20):
    try:
        p = remote(host, port)
        # ... exploit assuming rand_offset=0 ...
        flag = p.recvline(timeout=5)
        if b'FLAG' in flag or b'DH{' in flag:
            print(f"FLAG: {flag}")
            break
    except:
        p.close()
```

#### C. Padding Chunk Consumption
Random padding allocations pollute the heap but follow predictable patterns:
- Padding chunks are allocated then **never freed** → they accumulate
- After many operations, heap becomes fragmented → **top chunk exhausted** → mmap fallback
- **Strategy**: Fill heap until allocator behavior becomes predictable

#### D. Alignment Mask Exploitation
`free(ptr & MASK)` can free a DIFFERENT chunk than intended:
- If attacker controls `ptr` (e.g., via UAF write), crafted pointer → free arbitrary chunk
- Combine with info leak to target specific chunks

## 3. Fully Custom Allocators (Sand_Message pattern)

### Structure
```
mmap region (large, randomized base):
  ┌──────────────────────────────────────────┐
  │ [header: prev_size | curr_size | flags]  │
  │ [user data ...........................]   │
  │ [header: prev_size | curr_size | flags]  │
  │ [user data ...........................]   │
  └──────────────────────────────────────────┘
Free chunks tracked in bins (RB-tree / linked list / bitmap)
```

### Attack Strategies

#### A. Metadata Corruption (Unlink Attack Variant)
If allocator coalesces free chunks by reading metadata from adjacent chunks:
```
Target: Corrupt prev_size/curr_size of adjacent chunk
Effect: Coalescing creates overlapping chunk → overlap two live allocations
```
1. Allocate A, B, C contiguously
2. Free A (goes to free list)
3. Overflow from B into C's header → fake prev_size pointing to A
4. Free C → coalesce with "A" → giant free chunk overlapping B
5. Allocate over B → control B's data

#### B. UAF via Missing Zeroing
If allocator doesn't clear freed chunk data (Sand_Message pattern):
- Free chunk → data persists → reallocate same size → **read stale pointers**
- **Info leak**: Free struct with pointer field → reallocate → read pointer
- **Control flow hijack**: Free struct with function pointer → reallocate → overwrite → trigger call

#### C. Best-Fit vs First-Fit Exploitation
```
| Strategy | First-Fit | Best-Fit |
|----------|-----------|----------|
| LIFO reuse | Free A, alloc same size → get A | Not guaranteed |
| Size precision | Approximate OK | Exact size match preferred |
| Heap grooming | Simpler (predictable) | Need exact size control |
```
- **Best-fit (Sand_Message)**: Allocations routed by `log2(size/0x20)` to bins → need EXACT size class match
- **First-fit**: Free order matters → LIFO (last freed = first reallocated)

#### D. Bin Confusion
If size-to-bin mapping has edge cases:
- Find two different request sizes that map to same bin
- Free object of size A → reallocate with size B from same bin → type confusion

## 4. UAF Exploitation Patterns (Common to Both Types)

### Pattern 1: Dangling Pointer + Reallocation
```
1. Allocate victim struct (contains function ptr or data ptr)
2. Free victim (pointer NOT nulled → dangling)
3. Allocate new object of SAME chunk size → occupies victim's memory
4. Trigger use of dangling pointer → reads attacker-controlled data
```
**hunter**: `game_over_man` frees player but `player_ptr` stays → `change_player` writes to freed memory

### Pattern 2: UAF Write → Fake Structure
```
1. Free victim struct
2. Write to freed memory via dangling pointer
3. Craft fake structure:
   - Fake vtable/function pointer → code execution
   - Fake data pointer → arbitrary read/write
4. Trigger operation that uses the fake structure
```
**hunter**: `change_player` writes 8-byte name + sets sub_struct pointer on freed player

### Pattern 3: Linked List Hijack
If freed/reallocated struct contains linked list pointer:
```
1. UAF write to overwrite list head/next pointer
2. Point to GOT entry, stack, or .bss target
3. List traversal reads/writes to arbitrary address
```
**hunter**: player→sub_struct→list_head → item traversal → if list_head is fake, arbitrary memory access

## 5. Info Leak Techniques

### Heap Address Leak
- Print freed chunk content (may contain heap pointers from free list)
- **hunter**: `"black sheep wall"` prints `player_ptr` content after free → heap pointers

### Libc Address Leak
- Free chunk into unsorted bin (size > fastbin max, typically > 0x80 on 32-bit, > 0x408 on 64-bit with tcache)
- Unsorted bin fd/bk point to `main_arena` in libc
- For custom allocators: look for any pointer stored in freed chunk that references libc

### Stack/Code Leak
- Format string bugs (if any printf with user data)
- Partial overwrite techniques (bypass ASLR with known offsets)

## 6. Exploit Development Workflow

### Phase 1: Heap Grooming
```python
# Fill heap to predictable state
for i in range(N):
    alloc(fixed_size)  # consume random padding effects
```

### Phase 2: UAF Setup
```python
alloc_victim()         # target struct
# ... meet conditions for free ...
free_victim()          # dangling pointer remains
```

### Phase 3: Reallocation & Control
```python
# Allocate with SAME chunk size as victim
alloc_replacement(crafted_data)  # occupies victim's memory
```

### Phase 4: Trigger
```python
use_victim()  # uses dangling pointer → reads crafted_data
# → arbitrary read/write/exec depending on struct layout
```

### Phase 5: Escalate
```python
# GOT overwrite, vtable hijack, or direct shell trigger
# Depends on available primitives from Phase 4
```

## 7. Debugging Tips

### GDB Heap Visualization
```python
# Custom allocator — can't use standard heap commands
# Manual chunk walking:
define walk_heap
  set $p = $arg0
  set $end = $arg1
  while $p < $end
    printf "Chunk @ %p: prev_size=%x, size=%x\n", $p, *(int*)$p, *(int*)($p+4)
    set $p = $p + (*(int*)($p+4) & ~0x7)
  end
end
```

### Allocation Tracing
```python
# In solve.py: trace every allocation
import os
os.environ['MALLOC_TRACE'] = '/tmp/mtrace.log'
# OR: GDB breakpoints on custom_malloc/custom_free
```

### Crash Analysis
```
SIGSEGV in free() → corrupted chunk metadata
SIGSEGV in malloc() → corrupted free list
SIGABRT → glibc assertion (double free, corrupted size, etc.)
```

## 8. Challenges Reference

| Challenge | Allocator | Key Primitive | Status | Difficulty |
|-----------|-----------|---------------|--------|------------|
| hunter (pwnable.kr) | Randomized wrapper (rdtsc + offset) | UAF via "game over man" + change_player write | IN PROGRESS | Grotesque |
| Sand_Message (Dreamhack) | mmap custom (RB-tree bins, 16B header) | UAF (no zeroing on free) | IN PROGRESS | Hard |

## 9. Key Takeaways

1. **Custom allocator ≠ no exploitation** — they remove glibc checks but introduce new primitives
2. **Size class matching is critical** — wrong size = wrong bin = no reuse
3. **Probabilistic attacks are viable** — 25% success per attempt = ~4 tries average
4. **Reverse the allocator FULLY before attempting exploit** — chunk sizes, metadata, bin assignment, coalescing rules
5. **Trace allocations in GDB** — custom allocators are opaque to standard heap inspection tools
6. **Heap grooming is mandatory** — random padding/offsets require many allocations to reach predictable state
